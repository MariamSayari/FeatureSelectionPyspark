{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create session\n",
    "appName = \"Feature selection in Spark\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(appName) \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "#from pyspark.sql import HiveContext\n",
    "import pyspark\n",
    "from IPython.display import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(n_clusters=3,\n",
    "                 n_samples=20,\n",
    "                 n_definingfeatures=2,\n",
    "                 n_shuffle_defining_features=0,\n",
    "                 n_uniform_features=0,\n",
    "                 n_normal_features=0):\n",
    "    data_x, data_y = make_blobs(n_samples=n_samples, centers=n_clusters, random_state=42, n_features=n_definingfeatures, \n",
    "                               cluster_std=0.05, center_box=(0, 1))\n",
    "\n",
    "    n_non_shuffle_dims= np.arange(0,n_definingfeatures-n_shuffle_defining_features)\n",
    "\n",
    "    non_shuffled_data = data_x[:, n_non_shuffle_dims ]\n",
    "    shuffle_data = data_x[:, len(n_non_shuffle_dims): ]\n",
    "\n",
    "    uniform_shuffle = np.random.uniform(low=0, high=1, size=(n_samples, n_shuffle_defining_features,))\n",
    "    # shuffle the dimensions for each cluster\n",
    "    dims = np.repeat(np.arange(n_shuffle_defining_features *2).reshape(1,-1),n_clusters, axis=0)\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        np.random.shuffle(dims[i])\n",
    "    print(dims)\n",
    "    cluster_shuffled_features = np.stack([dims[i] for i in data_y])\n",
    "    mixed_data_to_shuffle =np.hstack([shuffle_data, uniform_shuffle])\n",
    "\n",
    "    mixed_shuffled_data = np.zeros_like(mixed_data_to_shuffle)\n",
    "    for i in range(len(mixed_data_to_shuffle)):\n",
    "        mixed_shuffled_data[i] = mixed_data_to_shuffle[i, :][cluster_shuffled_features[i]]\n",
    "\n",
    "    uniform_features = np.random.uniform(low=0.1, high=0.9, size=(n_samples, n_uniform_features,))\n",
    "    normal_features = np.random.normal(size=(n_samples, n_normal_features,))\n",
    "\n",
    "\n",
    "    data = np.concatenate([non_shuffled_data, mixed_shuffled_data, uniform_features,normal_features], axis=1)\n",
    "    print(f'discriminative non_shuffled_data {non_shuffled_data.shape}, disc mixed_shuffled_data {mixed_shuffled_data.shape} ' +\n",
    "         f'uniform_features {uniform_features.shape}, normal_features {normal_features.shape}')\n",
    "    data = preprocessing.MinMaxScaler().fit_transform(data)\n",
    "    return data, data_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data having a combination of distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 4 3 0 5]\n",
      " [2 3 0 4 1 5]\n",
      " [2 3 1 4 0 5]\n",
      " [2 1 0 5 4 3]]\n",
      "discriminative non_shuffled_data (200, 2), disc mixed_shuffled_data (200, 6) uniform_features (200, 1), normal_features (200, 1)\n",
      "(200, 10)\n"
     ]
    }
   ],
   "source": [
    "n_clusters = 4\n",
    "n_samples=200\n",
    "n_definingfeatures=5\n",
    "n_shuffle_defining_features=3\n",
    "n_uniform_features=1\n",
    "n_normal_features=1\n",
    "minClusterSize = 10 # parameter used for pruning the tree\n",
    "\n",
    "data, truth = generateData(n_clusters=n_clusters,\n",
    "                 n_samples=n_samples,\n",
    "                 n_definingfeatures=n_definingfeatures,\n",
    "                 n_shuffle_defining_features=n_shuffle_defining_features,\n",
    "                 n_uniform_features=n_uniform_features,\n",
    "                 n_normal_features=n_normal_features)\n",
    "N, M = data.shape\n",
    "print( data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter low-variance features - Variance Threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.feature_selection\n",
    "selector = sklearn.feature_selection.VarianceThreshold()\n",
    "selector.fit_transform(data)[1].size #Tous les features sont maintenus car ils n'ont pas les mÃªme valeurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean absolute difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mad = np.sum(np.abs(data -np.mean(data, axis =0 )), axis = 0)/data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAARR0lEQVR4nO3dbYwdZ3nG8f9VuwkFBA3NfqlfsAFDMaVNqsWhRQQJAhilivkQhEGgUKWyUsWFNq2KKShURlQhVJR+MCUWuEJAaiCgalVM04gAEkIBb15KcFIrG5PGi1Ox4BSqQhM23P2wk+pkOc6OvS8nefb/k1aeeV5m7omda2fnzMymqpAkteuXRl2AJGl5GfSS1DiDXpIaZ9BLUuMMeklq3NpRFzDfueeeW5s2bRp1GZL0pHLrrbf+oKrGhvU94YJ+06ZNTE5OjroMSXpSSfIfp+rrdekmyfYkR5NMJdkzpP+KJHcmuSPJ15Ns7do3Jflp135Hko+e+WFIks7Egmf0SdYA+4BXA9PA4SQTVXXXwLDrq+qj3fhLgA8B27u+e6vqvKUtW5LUV58z+m3AVFUdq6qHgYPAjsEBVfXjgdWnAT5uK0lPEH2Cfh1wfGB9umt7jCRXJrkXuBZ4+0DX5iS3J/lakpcP20GSXUkmk0zOzMycRvmSpIX0CfoMafuFM/aq2ldVzwXeCbyna34A2FhV5wNXAdcnecaQufuraryqxsfGhn5oLEk6Q32CfhrYMLC+HjjxOOMPAq8HqKqHquqH3fKtwL3A88+sVEnSmegT9IeBLUk2JzkL2AlMDA5IsmVg9WLgnq59rPswlyTPAbYAx5aicElSPwvedVNVs0l2AzcCa4ADVXUkyV5gsqomgN1JLgJ+BjwIXNZNvxDYm2QWeAS4oqpOLseBSJKGyxPtffTj4+PlA1OSdHqS3FpV48P6nnBPxj6ZbdrzxWXfx33XXLzs+5DUFl9qJkmNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxvUK+iTbkxxNMpVkz5D+K5LcmeSOJF9PsnWg713dvKNJXruUxUuSFrZg0CdZA+wDXgdsBd40GOSd66vqxVV1HnAt8KFu7lZgJ/AiYDvwkW57kqQV0ueMfhswVVXHquph4CCwY3BAVf14YPVpQHXLO4CDVfVQVX0XmOq2J0laIWt7jFkHHB9YnwYumD8oyZXAVcBZwCsH5t4yb+66IXN3AbsANm7c2KduSVJPfc7oM6StfqGhal9VPRd4J/Ce05y7v6rGq2p8bGysR0mSpL76BP00sGFgfT1w4nHGHwRef4ZzJUlLrE/QHwa2JNmc5CzmPlydGByQZMvA6sXAPd3yBLAzydlJNgNbgG8tvmxJUl8LXqOvqtkku4EbgTXAgao6kmQvMFlVE8DuJBcBPwMeBC7r5h5J8lngLmAWuLKqHlmmY5EkDdHnw1iq6hBwaF7b1QPL73icue8H3n+mBUqSFscnYyWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9Jjev1i0ck6VGb9nxx2fdx3zUXL/s+VhPP6CWpcQa9JDXOoJekxhn0ktS4XkGfZHuSo0mmkuwZ0n9VkruSfDvJl5M8e6DvkSR3dF8TS1m8JGlhC951k2QNsA94NTANHE4yUVV3DQy7HRivqp8k+SPgWuCNXd9Pq+q8Ja5bktRTnzP6bcBUVR2rqoeBg8COwQFV9ZWq+km3eguwfmnLlCSdqT730a8Djg+sTwMXPM74y4EvDaw/JckkMAtcU1X/dNpVakHe2yzpVPoEfYa01dCByVuAceAVA80bq+pEkucANye5s6runTdvF7ALYOPGjb0KlyT10yfop4ENA+vrgRPzByW5CHg38IqqeujR9qo60f15LMlXgfOBxwR9Ve0H9gOMj48P/SYiSf7kemb6XKM/DGxJsjnJWcBO4DF3zyQ5H7gOuKSqvj/Qfk6Ss7vlc4GXAYMf4kqSltmCZ/RVNZtkN3AjsAY4UFVHkuwFJqtqAvgg8HTgc0kA7q+qS4AXAtcl+Tlz31SumXe3jvSk5dmlnix6vdSsqg4Bh+a1XT2wfNEp5n0DePFiCpQkLY5PxkpS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1Dh/Obie1HxoSVqYZ/SS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXG9gj7J9iRHk0wl2TOk/6okdyX5dpIvJ3n2QN9lSe7pvi5byuIlSQtbMOiTrAH2Aa8DtgJvSrJ13rDbgfGq+i3gBuDabu6zgPcCFwDbgPcmOWfpypckLaTPGf02YKqqjlXVw8BBYMfggKr6SlX9pFu9BVjfLb8WuKmqTlbVg8BNwPalKV2S1EefoF8HHB9Yn+7aTuVy4EunMzfJriSTSSZnZmZ6lCRJ6qtP0GdIWw0dmLwFGAc+eDpzq2p/VY1X1fjY2FiPkiRJffUJ+mlgw8D6euDE/EFJLgLeDVxSVQ+dzlxJ0vLp88vBDwNbkmwGvgfsBN48OCDJ+cB1wPaq+v5A143AXw98APsa4F2Lrvpx+MuiJemxFgz6qppNspu50F4DHKiqI0n2ApNVNcHcpZqnA59LAnB/VV1SVSeTvI+5bxYAe6vq5LIciSRpqD5n9FTVIeDQvLarB5Yvepy5B4ADZ1qgJGlxfDJWkhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXG9gj7J9iRHk0wl2TOk/8IktyWZTXLpvL5HktzRfU0sVeGSpH7WLjQgyRpgH/BqYBo4nGSiqu4aGHY/8Dbgz4ds4qdVdd4S1KonqE17vrjs+7jvmouXfR9SqxYMemAbMFVVxwCSHAR2AP8f9FV1X9f382WoUZK0CH0u3awDjg+sT3dtfT0lyWSSW5K8ftiAJLu6MZMzMzOnsWlJ0kL6BH2GtNVp7GNjVY0DbwY+nOS5v7Cxqv1VNV5V42NjY6exaUnSQvoE/TSwYWB9PXCi7w6q6kT35zHgq8D5p1GfJGmR+gT9YWBLks1JzgJ2Ar3unklyTpKzu+VzgZcxcG1fkrT8Fgz6qpoFdgM3AncDn62qI0n2JrkEIMlLkkwDbwCuS3Kkm/5CYDLJvwFfAa6Zd7eOJGmZ9bnrhqo6BBya13b1wPJh5i7pzJ/3DeDFi6xRkrQIvYJe0hOLzy6svCfzf3NfgSBJjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUuF5Bn2R7kqNJppLsGdJ/YZLbkswmuXRe32VJ7um+LluqwiVJ/SwY9EnWAPuA1wFbgTcl2Tpv2P3A24Dr5819FvBe4AJgG/DeJOcsvmxJUl99zui3AVNVdayqHgYOAjsGB1TVfVX1beDn8+a+Fripqk5W1YPATcD2JahbktRTn6BfBxwfWJ/u2vroNTfJriSTSSZnZmZ6blqS1EefoM+Qtuq5/V5zq2p/VY1X1fjY2FjPTUuS+ugT9NPAhoH19cCJnttfzFxJ0hLoE/SHgS1JNic5C9gJTPTc/o3Aa5Kc030I+5quTZK0QhYM+qqaBXYzF9B3A5+tqiNJ9ia5BCDJS5JMA28ArktypJt7Engfc98sDgN7uzZJ0gpZ22dQVR0CDs1ru3pg+TBzl2WGzT0AHFhEjZKkRfDJWElqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TG9Qr6JNuTHE0ylWTPkP6zk3ym6/9mkk1d+6YkP01yR/f10aUtX5K0kLULDUiyBtgHvBqYBg4nmaiquwaGXQ48WFXPS7IT+ADwxq7v3qo6b4nrliT11OeMfhswVVXHquph4CCwY96YHcAnuuUbgFclydKVKUk6U32Cfh1wfGB9umsbOqaqZoEfAb/W9W1OcnuSryV5+bAdJNmVZDLJ5MzMzGkdgCTp8fUJ+mFn5tVzzAPAxqo6H7gKuD7JM35hYNX+qhqvqvGxsbEeJUmS+uoT9NPAhoH19cCJU41JshZ4JnCyqh6qqh8CVNWtwL3A8xdbtCSpvz5BfxjYkmRzkrOAncDEvDETwGXd8qXAzVVVSca6D3NJ8hxgC3BsaUqXJPWx4F03VTWbZDdwI7AGOFBVR5LsBSaragL4OPDJJFPASea+GQBcCOxNMgs8AlxRVSeX40AkScMtGPQAVXUIODSv7eqB5f8F3jBk3ueBzy+yRknSIvhkrCQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJalyvoE+yPcnRJFNJ9gzpPzvJZ7r+bybZNND3rq79aJLXLl3pkqQ+Fgz6JGuAfcDrgK3Am5JsnTfscuDBqnoe8LfAB7q5W4GdwIuA7cBHuu1JklZInzP6bcBUVR2rqoeBg8COeWN2AJ/olm8AXpUkXfvBqnqoqr4LTHXbkyStkFTV4w9ILgW2V9UfdutvBS6oqt0DY77TjZnu1u8FLgD+Crilqj7VtX8c+FJV3TBvH7uAXd3qC4Cjiz+03s4FfrCC+3ui8LhXl9V63LB6jv3ZVTU2rGNtj8kZ0jb/u8OpxvSZS1XtB/b3qGXJJZmsqvFR7HuUPO7VZbUeN6zuY39Un0s308CGgfX1wIlTjUmyFngmcLLnXEnSMuoT9IeBLUk2JzmLuQ9XJ+aNmQAu65YvBW6uuWtCE8DO7q6czcAW4FtLU7okqY8FL91U1WyS3cCNwBrgQFUdSbIXmKyqCeDjwCeTTDF3Jr+zm3skyWeBu4BZ4MqqemSZjuVMjeSS0ROAx726rNbjhtV97ECPD2MlSU9uPhkrSY0z6CWpcas66Bd6tUOLkmxI8pUkdyc5kuQdo65pJSVZk+T2JP886lpWSpJfTXJDkn/v/t5/d9Q1rYQkf9r9G/9Okn9M8pRR1zQqqzboe77aoUWzwJ9V1QuBlwJXrpLjftQ7gLtHXcQK+zvgX6rqN4DfZhUcf5J1wNuB8ar6TeZuJNk52qpGZ9UGPf1e7dCcqnqgqm7rlv+buf/p1422qpWRZD1wMfCxUdeyUpI8A7iQuTvjqKqHq+q/RlvVilkL/Er3bM9TWcXP8KzmoF8HHB9Yn2aVBN6jureMng98c7SVrJgPA38B/HzUhayg5wAzwD90l6w+luRpoy5quVXV94C/Ae4HHgB+VFX/OtqqRmc1B32v1zO0KsnTgc8Df1JVPx51Pcstye8D36+qW0ddywpbC/wO8PdVdT7wP0Dzn0clOYe5n9A3A78OPC3JW0Zb1eis5qBfta9nSPLLzIX8p6vqC6OuZ4W8DLgkyX3MXaZ7ZZJPjbakFTENTFfVoz+13cBc8LfuIuC7VTVTVT8DvgD83ohrGpnVHPR9Xu3QnO710R8H7q6qD426npVSVe+qqvVVtYm5v+ubq6r5M7yq+k/geJIXdE2vYu5J9dbdD7w0yVO7f/OvYhV8CH0qfd5e2aRTvdphxGWthJcBbwXuTHJH1/aXVXVohDVpef0x8OnuhOYY8AcjrmfZVdU3k9wA3MbcnWa3s4pfheArECSpcav50o0krQoGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWrc/wGB1vPL1rcKugAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(np.arange(data.shape[1]),mad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dispesion ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispersion(data):\n",
    "    data = data +1 #avoid 0 division\n",
    "    aritmeticMean = np.mean(data, axis =0 )\n",
    "    geometricMean = np.power(np.prod(data, axis =0 ),1/data.shape[0])\n",
    "    R = aritmeticMean/geometricMean\n",
    "    return R\n",
    "R = dispersion(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(np.arange(data.shape[1]),R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplacian Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def construct_W(X, neighbour_size = 5, t = 1):\n",
    "    n_samples, n_features = np.shape(X)\n",
    "    S=kneighbors_graph(X, neighbour_size+1, mode='distance',metric='euclidean') #sqecludian distance works only with mode=connectivity  results were absurd\n",
    "    S = (-1*(S*S))/(2*t*t)\n",
    "    S=S.tocsc()\n",
    "    S=expm(S) # exponential\n",
    "    S=S.tocsr()\n",
    "    #[1]  M. Belkin and P. Niyogi, âLaplacian Eigenmaps and Spectral Techniques for Embedding and Clustering,â Advances in Neural Information Processing Systems,\n",
    "    #Vol. 14, 2001. Following the paper to make the weights matrix symmetrix we use this method\n",
    "    bigger = np.transpose(S) > S\n",
    "    S = S - S.multiply(bigger) + np.transpose(S).multiply(bigger)\n",
    "    return S\n",
    "\n",
    "def LaplacianScore(X, neighbour_size = 5,  t = 1):\n",
    "    W = construct_W(X,t=t,neighbour_size=neighbour_size)\n",
    "    n_samples, n_features = np.shape(X)\n",
    "    \n",
    "    #construct the diagonal matrix\n",
    "    D=np.array(W.sum(axis=1))\n",
    "    D = scipy.sparse.diags(np.transpose(D), [0])\n",
    "    #construct graph Laplacian L\n",
    "    L=D-W.toarray()\n",
    "\n",
    "    #construct 1= [1,Â·Â·Â·,1]' \n",
    "    I=np.ones((n_samples,n_features))\n",
    "\n",
    "    #construct fr' => fr= [fr1,...,frn]'\n",
    "    Xt = np.transpose(X)\n",
    "\n",
    "    #construct fr^=fr-(frt D I/It D I)I\n",
    "    t=np.matmul(np.matmul(Xt,D.toarray()),I)/np.matmul(np.matmul(np.transpose(I),D.toarray()),I)\n",
    "    t=t[:,0]\n",
    "    t=np.tile(t,(n_samples,1))\n",
    "    fr=X-t\n",
    "\n",
    "    #Compute Laplacian Score\n",
    "    fr_t=np.transpose(fr)\n",
    "    Lr=np.matmul(np.matmul(fr_t,L),fr)/np.matmul(np.dot(fr_t,D.toarray()),fr)\n",
    "\n",
    "    return np.diag(Lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import kneighbors_graph \n",
    "import scipy\n",
    "import scipy.sparse \n",
    "from scipy.sparse.linalg import expm\n",
    "import scipy.spatial.distance\n",
    "ls =LaplacianScore(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Laplacian feature importance scores')\n",
    "plt.bar(np.arange(data.shape[1]),ls);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplacian Score combined with distance-based entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distanceEntropy(d, mu = 0.5, beta=10):\n",
    "    \"\"\"\n",
    "    As per: An Unsupervised Feature Selection Algorithm: Laplacian Score Combined with\n",
    "    Distance-based Entropy Measure, Rongye Liu \n",
    "    \"\"\"\n",
    "    if d<=mu:\n",
    "        result = (np.exp(beta * d) - np.exp(0))/(np.exp(beta * mu) - np.exp(0))\n",
    "    else:\n",
    "        result = (np.exp(beta * (1-d) )- np.exp(0))/(np.exp(beta *(1- mu)) - np.exp(0))              \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lse(data, ls):\n",
    "    \"\"\"\n",
    "    This method takes as input a dataset, its laplacian scores for all features\n",
    "    and applies distance based entropy feature selection in order to identify\n",
    "    the best subset of features in the laplacian sense.\n",
    "    \"\"\"\n",
    "    orderedFeatures = np.argsort(ls)\n",
    "    scores = {}\n",
    "    for i in range (2,len(ls)):\n",
    "        selectedFeatures = orderedFeatures[:i]\n",
    "        selectedFeaturesDataset = data[:, selectedFeatures]\n",
    "        d =sklearn.metrics.pairwise_distances(selectedFeaturesDataset, metric = 'euclidean' )\n",
    "        beta =10\n",
    "        mu = 0.5\n",
    "\n",
    "        d = preprocessing.MinMaxScaler().fit_transform(d)\n",
    "        e = np.vectorize(distanceEntropy)(d) \n",
    "        e = preprocessing.MinMaxScaler().fit_transform(e)\n",
    "        totalEntropy= np.sum(e)\n",
    "        scores[i] = totalEntropy\n",
    "    bestFeatures = orderedFeatures[:list(scores.keys())[np.argmin(scores.values())]]\n",
    "    return bestFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Cluster Feature selection MCFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import *\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "\n",
    "def construct_W(X, **kwargs):\n",
    "    \"\"\"\n",
    "    Construct the affinity matrix W through different ways\n",
    "    Notes\n",
    "    -----\n",
    "    if kwargs is null, use the default parameter settings;\n",
    "    if kwargs is not null, construct the affinity matrix according to parameters in kwargs\n",
    "    Input\n",
    "    -----\n",
    "    X: {numpy array}, shape (n_samples, n_features)\n",
    "        input data\n",
    "    kwargs: {dictionary}\n",
    "        parameters to construct different affinity matrix W:\n",
    "        y: {numpy array}, shape (n_samples, 1)\n",
    "            the true label information needed under the 'supervised' neighbor mode\n",
    "        metric: {string}\n",
    "            choices for different distance measures\n",
    "            'euclidean' - use euclidean distance\n",
    "            'cosine' - use cosine distance (default)\n",
    "        neighbor_mode: {string}\n",
    "            indicates how to construct the graph\n",
    "            'knn' - put an edge between two nodes if and only if they are among the\n",
    "                    k nearest neighbors of each other (default)\n",
    "            'supervised' - put an edge between two nodes if they belong to same class\n",
    "                    and they are among the k nearest neighbors of each other\n",
    "        weight_mode: {string}\n",
    "            indicates how to assign weights for each edge in the graph\n",
    "            'binary' - 0-1 weighting, every edge receives weight of 1 (default)\n",
    "            'heat_kernel' - if nodes i and j are connected, put weight W_ij = exp(-norm(x_i - x_j)/2t^2)\n",
    "                            this weight mode can only be used under 'euclidean' metric and you are required\n",
    "                            to provide the parameter t\n",
    "            'cosine' - if nodes i and j are connected, put weight cosine(x_i,x_j).\n",
    "                        this weight mode can only be used under 'cosine' metric\n",
    "        k: {int}\n",
    "            choices for the number of neighbors (default k = 5)\n",
    "        t: {float}\n",
    "            parameter for the 'heat_kernel' weight_mode\n",
    "        fisher_score: {boolean}\n",
    "            indicates whether to build the affinity matrix in a fisher score way, in which W_ij = 1/n_l if yi = yj = l;\n",
    "            otherwise W_ij = 0 (default fisher_score = false)\n",
    "        reliefF: {boolean}\n",
    "            indicates whether to build the affinity matrix in a reliefF way, NH(x) and NM(x,y) denotes a set of\n",
    "            k nearest points to x with the same class as x, and a different class (the class y), respectively.\n",
    "            W_ij = 1 if i = j; W_ij = 1/k if x_j \\in NH(x_i); W_ij = -1/(c-1)k if x_j \\in NM(x_i, y) (default reliefF = false)\n",
    "    Output\n",
    "    ------\n",
    "    W: {sparse matrix}, shape (n_samples, n_samples)\n",
    "        output affinity matrix W\n",
    "    \"\"\"\n",
    "\n",
    "    # default metric is 'cosine'\n",
    "    if 'metric' not in kwargs.keys():\n",
    "        kwargs['metric'] = 'cosine'\n",
    "\n",
    "    # default neighbor mode is 'knn' and default neighbor size is 5\n",
    "    if 'neighbor_mode' not in kwargs.keys():\n",
    "        kwargs['neighbor_mode'] = 'knn'\n",
    "    if kwargs['neighbor_mode'] == 'knn' and 'k' not in kwargs.keys():\n",
    "        kwargs['k'] = 5\n",
    "    if kwargs['neighbor_mode'] == 'supervised' and 'k' not in kwargs.keys():\n",
    "        kwargs['k'] = 5\n",
    "    if kwargs['neighbor_mode'] == 'supervised' and 'y' not in kwargs.keys():\n",
    "        print ('Warning: label is required in the supervised neighborMode!!!')\n",
    "        exit(0)\n",
    "\n",
    "    # default weight mode is 'binary', default t in heat kernel mode is 1\n",
    "    if 'weight_mode' not in kwargs.keys():\n",
    "        kwargs['weight_mode'] = 'binary'\n",
    "    if kwargs['weight_mode'] == 'heat_kernel':\n",
    "        if kwargs['metric'] != 'euclidean':\n",
    "            kwargs['metric'] = 'euclidean'\n",
    "        if 't' not in kwargs.keys():\n",
    "            kwargs['t'] = 1\n",
    "    elif kwargs['weight_mode'] == 'cosine':\n",
    "        if kwargs['metric'] != 'cosine':\n",
    "            kwargs['metric'] = 'cosine'\n",
    "\n",
    "    # default fisher_score and reliefF mode are 'false'\n",
    "    if 'fisher_score' not in kwargs.keys():\n",
    "        kwargs['fisher_score'] = False\n",
    "    if 'reliefF' not in kwargs.keys():\n",
    "        kwargs['reliefF'] = False\n",
    "\n",
    "    n_samples, n_features = np.shape(X)\n",
    "\n",
    "    # choose 'knn' neighbor mode\n",
    "    if kwargs['neighbor_mode'] == 'knn':\n",
    "        k = kwargs['k']\n",
    "        if kwargs['weight_mode'] == 'binary':\n",
    "            if kwargs['metric'] == 'euclidean':\n",
    "                # compute pairwise euclidean distances\n",
    "                D = pairwise_distances(X)\n",
    "                D **= 2\n",
    "                # sort the distance matrix D in ascending order\n",
    "                dump = np.sort(D, axis=1)\n",
    "                idx = np.argsort(D, axis=1)\n",
    "                # choose the k-nearest neighbors for each instance\n",
    "                idx_new = idx[:, 0:k+1]\n",
    "                G = np.zeros((n_samples*(k+1), 3))\n",
    "                G[:, 0] = np.tile(np.arange(n_samples), (k+1, 1)).reshape(-1)\n",
    "                G[:, 1] = np.ravel(idx_new, order='F')\n",
    "                G[:, 2] = 1\n",
    "                # build the sparse affinity matrix W\n",
    "                W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "                bigger = np.transpose(W) > W\n",
    "                W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n",
    "                return W\n",
    "\n",
    "            elif kwargs['metric'] == 'cosine':\n",
    "                # normalize the data first\n",
    "                X_normalized = np.power(np.sum(X*X, axis=1), 0.5)\n",
    "                for i in range(n_samples):\n",
    "                    X[i, :] = X[i, :]/max(1e-12, X_normalized[i])\n",
    "                # compute pairwise cosine distances\n",
    "                D_cosine = np.dot(X, np.transpose(X))\n",
    "                # sort the distance matrix D in descending order\n",
    "                dump = np.sort(-D_cosine, axis=1)\n",
    "                idx = np.argsort(-D_cosine, axis=1)\n",
    "                idx_new = idx[:, 0:k+1]\n",
    "                G = np.zeros((n_samples*(k+1), 3))\n",
    "                G[:, 0] = np.tile(np.arange(n_samples), (k+1, 1)).reshape(-1)\n",
    "                G[:, 1] = np.ravel(idx_new, order='F')\n",
    "                G[:, 2] = 1\n",
    "                # build the sparse affinity matrix W\n",
    "                W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "                bigger = np.transpose(W) > W\n",
    "                W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n",
    "                return W\n",
    "\n",
    "        elif kwargs['weight_mode'] == 'heat_kernel':\n",
    "            t = kwargs['t']\n",
    "            # compute pairwise euclidean distances\n",
    "            D = pairwise_distances(X)\n",
    "            D **= 2\n",
    "            # sort the distance matrix D in ascending order\n",
    "            dump = np.sort(D, axis=1)\n",
    "            idx = np.argsort(D, axis=1)\n",
    "            idx_new = idx[:, 0:k+1]\n",
    "            dump_new = dump[:, 0:k+1]\n",
    "            # compute the pairwise heat kernel distances\n",
    "            dump_heat_kernel = np.exp(-dump_new/(2*t*t))\n",
    "            G = np.zeros((n_samples*(k+1), 3))\n",
    "            G[:, 0] = np.tile(np.arange(n_samples), (k+1, 1)).reshape(-1)\n",
    "            G[:, 1] = np.ravel(idx_new, order='F')\n",
    "            G[:, 2] = np.ravel(dump_heat_kernel, order='F')\n",
    "            # build the sparse affinity matrix W\n",
    "            W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "            bigger = np.transpose(W) > W\n",
    "            W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n",
    "            return W\n",
    "\n",
    "        elif kwargs['weight_mode'] == 'cosine':\n",
    "            # normalize the data first\n",
    "            X_normalized = np.power(np.sum(X*X, axis=1), 0.5)\n",
    "            for i in range(n_samples):\n",
    "                    X[i, :] = X[i, :]/max(1e-12, X_normalized[i])\n",
    "            # compute pairwise cosine distances\n",
    "            D_cosine = np.dot(X, np.transpose(X))\n",
    "            # sort the distance matrix D in ascending order\n",
    "            dump = np.sort(-D_cosine, axis=1)\n",
    "            idx = np.argsort(-D_cosine, axis=1)\n",
    "            idx_new = idx[:, 0:k+1]\n",
    "            dump_new = -dump[:, 0:k+1]\n",
    "            G = np.zeros((n_samples*(k+1), 3))\n",
    "            G[:, 0] = np.tile(np.arange(n_samples), (k+1, 1)).reshape(-1)\n",
    "            G[:, 1] = np.ravel(idx_new, order='F')\n",
    "            G[:, 2] = np.ravel(dump_new, order='F')\n",
    "            # build the sparse affinity matrix W\n",
    "            W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "            bigger = np.transpose(W) > W\n",
    "            W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n",
    "            return W\n",
    "\n",
    "    # choose supervised neighborMode\n",
    "    elif kwargs['neighbor_mode'] == 'supervised':\n",
    "        k = kwargs['k']\n",
    "        # get true labels and the number of classes\n",
    "        y = kwargs['y']\n",
    "        label = np.unique(y)\n",
    "        n_classes = np.unique(y).size\n",
    "        # construct the weight matrix W in a fisherScore way, W_ij = 1/n_l if yi = yj = l, otherwise W_ij = 0\n",
    "        if kwargs['fisher_score'] is True:\n",
    "            W = lil_matrix((n_samples, n_samples))\n",
    "            for i in range(n_classes):\n",
    "                class_idx = (y == label[i])\n",
    "                class_idx_all = (class_idx[:, np.newaxis] & class_idx[np.newaxis, :])\n",
    "                W[class_idx_all] = 1.0/np.sum(np.sum(class_idx))\n",
    "            return W\n",
    "\n",
    "        # construct the weight matrix W in a reliefF way, NH(x) and NM(x,y) denotes a set of k nearest\n",
    "        # points to x with the same class as x, a different class (the class y), respectively. W_ij = 1 if i = j;\n",
    "        # W_ij = 1/k if x_j \\in NH(x_i); W_ij = -1/(c-1)k if x_j \\in NM(x_i, y)\n",
    "        if kwargs['reliefF'] is True:\n",
    "            # when xj in NH(xi)\n",
    "            G = np.zeros((n_samples*(k+1), 3))\n",
    "            id_now = 0\n",
    "            for i in range(n_classes):\n",
    "                class_idx = np.column_stack(np.where(y == label[i]))[:, 0]\n",
    "                D = pairwise_distances(X[class_idx, :])\n",
    "                D **= 2\n",
    "                idx = np.argsort(D, axis=1)\n",
    "                idx_new = idx[:, 0:k+1]\n",
    "                n_smp_class = (class_idx[idx_new[:]]).size\n",
    "                if len(class_idx) <= k:\n",
    "                    k = len(class_idx) - 1\n",
    "                G[id_now:n_smp_class+id_now, 0] = np.tile(class_idx, (k+1, 1)).reshape(-1)\n",
    "                G[id_now:n_smp_class+id_now, 1] = np.ravel(class_idx[idx_new[:]], order='F')\n",
    "                G[id_now:n_smp_class+id_now, 2] = 1.0/k\n",
    "                id_now += n_smp_class\n",
    "            W1 = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "            # when i = j, W_ij = 1\n",
    "            for i in range(n_samples):\n",
    "                W1[i, i] = 1\n",
    "            # when x_j in NM(x_i, y)\n",
    "            G = np.zeros((n_samples*k*(n_classes - 1), 3))\n",
    "            id_now = 0\n",
    "            for i in range(n_classes):\n",
    "                class_idx1 = np.column_stack(np.where(y == label[i]))[:, 0]\n",
    "                X1 = X[class_idx1, :]\n",
    "                for j in range(n_classes):\n",
    "                    if label[j] != label[i]:\n",
    "                        class_idx2 = np.column_stack(np.where(y == label[j]))[:, 0]\n",
    "                        X2 = X[class_idx2, :]\n",
    "                        D = pairwise_distances(X1, X2)\n",
    "                        idx = np.argsort(D, axis=1)\n",
    "                        idx_new = idx[:, 0:k]\n",
    "                        n_smp_class = len(class_idx1)*k\n",
    "                        G[id_now:n_smp_class+id_now, 0] = np.tile(class_idx1, (k, 1)).reshape(-1)\n",
    "                        G[id_now:n_smp_class+id_now, 1] = np.ravel(class_idx2[idx_new[:]], order='F')\n",
    "                        G[id_now:n_smp_class+id_now, 2] = -1.0/((n_classes-1)*k)\n",
    "                        id_now += n_smp_class\n",
    "            W2 = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "            bigger = np.transpose(W2) > W2\n",
    "            W2 = W2 - W2.multiply(bigger) + np.transpose(W2).multiply(bigger)\n",
    "            W = W1 + W2\n",
    "            return W\n",
    "\n",
    "        if kwargs['weight_mode'] == 'binary':\n",
    "            if kwargs['metric'] == 'euclidean':\n",
    "                G = np.zeros((n_samples*(k+1), 3))\n",
    "                id_now = 0\n",
    "                for i in range(n_classes):\n",
    "                    class_idx = np.column_stack(np.where(y == label[i]))[:, 0]\n",
    "                    # compute pairwise euclidean distances for instances in class i\n",
    "                    D = pairwise_distances(X[class_idx, :])\n",
    "                    D **= 2\n",
    "                    # sort the distance matrix D in ascending order for instances in class i\n",
    "                    idx = np.argsort(D, axis=1)\n",
    "                    idx_new = idx[:, 0:k+1]\n",
    "                    n_smp_class = len(class_idx)*(k+1)\n",
    "                    G[id_now:n_smp_class+id_now, 0] = np.tile(class_idx, (k+1, 1)).reshape(-1)\n",
    "                    G[id_now:n_smp_class+id_now, 1] = np.ravel(class_idx[idx_new[:]], order='F')\n",
    "                    G[id_now:n_smp_class+id_now, 2] = 1\n",
    "                    id_now += n_smp_class\n",
    "                # build the sparse affinity matrix W\n",
    "                W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "                bigger = np.transpose(W) > W\n",
    "                W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n",
    "                return W\n",
    "\n",
    "            if kwargs['metric'] == 'cosine':\n",
    "                # normalize the data first\n",
    "                X_normalized = np.power(np.sum(X*X, axis=1), 0.5)\n",
    "                for i in range(n_samples):\n",
    "                    X[i, :] = X[i, :]/max(1e-12, X_normalized[i])\n",
    "                G = np.zeros((n_samples*(k+1), 3))\n",
    "                id_now = 0\n",
    "                for i in range(n_classes):\n",
    "                    class_idx = np.column_stack(np.where(y == label[i]))[:, 0]\n",
    "                    # compute pairwise cosine distances for instances in class i\n",
    "                    D_cosine = np.dot(X[class_idx, :], np.transpose(X[class_idx, :]))\n",
    "                    # sort the distance matrix D in descending order for instances in class i\n",
    "                    idx = np.argsort(-D_cosine, axis=1)\n",
    "                    idx_new = idx[:, 0:k+1]\n",
    "                    n_smp_class = len(class_idx)*(k+1)\n",
    "                    G[id_now:n_smp_class+id_now, 0] = np.tile(class_idx, (k+1, 1)).reshape(-1)\n",
    "                    G[id_now:n_smp_class+id_now, 1] = np.ravel(class_idx[idx_new[:]], order='F')\n",
    "                    G[id_now:n_smp_class+id_now, 2] = 1\n",
    "                    id_now += n_smp_class\n",
    "                # build the sparse affinity matrix W\n",
    "                W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "                bigger = np.transpose(W) > W\n",
    "                W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n",
    "                return W\n",
    "\n",
    "        elif kwargs['weight_mode'] == 'heat_kernel':\n",
    "            G = np.zeros((n_samples*(k+1), 3))\n",
    "            id_now = 0\n",
    "            for i in range(n_classes):\n",
    "                class_idx = np.column_stack(np.where(y == label[i]))[:, 0]\n",
    "                # compute pairwise cosine distances for instances in class i\n",
    "                D = pairwise_distances(X[class_idx, :])\n",
    "                D **= 2\n",
    "                # sort the distance matrix D in ascending order for instances in class i\n",
    "                dump = np.sort(D, axis=1)\n",
    "                idx = np.argsort(D, axis=1)\n",
    "                idx_new = idx[:, 0:k+1]\n",
    "                dump_new = dump[:, 0:k+1]\n",
    "                t = kwargs['t']\n",
    "                # compute pairwise heat kernel distances for instances in class i\n",
    "                dump_heat_kernel = np.exp(-dump_new/(2*t*t))\n",
    "                n_smp_class = len(class_idx)*(k+1)\n",
    "                G[id_now:n_smp_class+id_now, 0] = np.tile(class_idx, (k+1, 1)).reshape(-1)\n",
    "                G[id_now:n_smp_class+id_now, 1] = np.ravel(class_idx[idx_new[:]], order='F')\n",
    "                G[id_now:n_smp_class+id_now, 2] = np.ravel(dump_heat_kernel, order='F')\n",
    "                id_now += n_smp_class\n",
    "            # build the sparse affinity matrix W\n",
    "            W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "            bigger = np.transpose(W) > W\n",
    "            W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n",
    "            return W\n",
    "\n",
    "        elif kwargs['weight_mode'] == 'cosine':\n",
    "            # normalize the data first\n",
    "            X_normalized = np.power(np.sum(X*X, axis=1), 0.5)\n",
    "            for i in range(n_samples):\n",
    "                X[i, :] = X[i, :]/max(1e-12, X_normalized[i])\n",
    "            G = np.zeros((n_samples*(k+1), 3))\n",
    "            id_now = 0\n",
    "            for i in range(n_classes):\n",
    "                class_idx = np.column_stack(np.where(y == label[i]))[:, 0]\n",
    "                # compute pairwise cosine distances for instances in class i\n",
    "                D_cosine = np.dot(X[class_idx, :], np.transpose(X[class_idx, :]))\n",
    "                # sort the distance matrix D in descending order for instances in class i\n",
    "                dump = np.sort(-D_cosine, axis=1)\n",
    "                idx = np.argsort(-D_cosine, axis=1)\n",
    "                idx_new = idx[:, 0:k+1]\n",
    "                dump_new = -dump[:, 0:k+1]\n",
    "                n_smp_class = len(class_idx)*(k+1)\n",
    "                G[id_now:n_smp_class+id_now, 0] = np.tile(class_idx, (k+1, 1)).reshape(-1)\n",
    "                G[id_now:n_smp_class+id_now, 1] = np.ravel(class_idx[idx_new[:]], order='F')\n",
    "                G[id_now:n_smp_class+id_now, 2] = np.ravel(dump_new, order='F')\n",
    "                id_now += n_smp_class\n",
    "            # build the sparse affinity matrix W\n",
    "            W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n",
    "            bigger = np.transpose(W) > W\n",
    "            W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n",
    "            return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "\n",
    "def mcfs(X, n_selected_features, **kwargs):\n",
    "    \"\"\"\n",
    "    This function implements unsupervised feature selection for multi-cluster data.\n",
    "    Input\n",
    "    -----\n",
    "    X: {numpy array}, shape (n_samples, n_features)\n",
    "        input data\n",
    "    n_selected_features: {int}\n",
    "        number of features to select\n",
    "    kwargs: {dictionary}\n",
    "        W: {sparse matrix}, shape (n_samples, n_samples)\n",
    "            affinity matrix\n",
    "        n_clusters: {int}\n",
    "            number of clusters (default is 5)\n",
    "    Output\n",
    "    ------\n",
    "    W: {numpy array}, shape(n_features, n_clusters)\n",
    "        feature weight matrix\n",
    "    Reference\n",
    "    ---------\n",
    "    Cai, Deng et al. \"Unsupervised Feature Selection for Multi-Cluster Data.\" KDD 2010.\n",
    "    \"\"\"\n",
    "\n",
    "    # use the default affinity matrix\n",
    "    if 'W' not in kwargs:\n",
    "        W = construct_W(X)\n",
    "    else:\n",
    "        W = kwargs['W']\n",
    "    # default number of clusters is 5\n",
    "    if 'n_clusters' not in kwargs:\n",
    "        n_clusters = 5\n",
    "    else:\n",
    "        n_clusters = kwargs['n_clusters']\n",
    "\n",
    "    # solve the generalized eigen-decomposition problem and get the top K\n",
    "    # eigen-vectors with respect to the smallest eigenvalues\n",
    "    W = W.toarray()\n",
    "    W = (W + W.T) / 2\n",
    "    W_norm = np.diag(np.sqrt(1 / W.sum(1)))\n",
    "    W = np.dot(W_norm, np.dot(W, W_norm))\n",
    "    WT = W.T\n",
    "    W[W < WT] = WT[W < WT]\n",
    "    eigen_value, ul = scipy.linalg.eigh(a=W)\n",
    "    Y = np.dot(W_norm, ul[:, -1*n_clusters-1:-1])\n",
    "\n",
    "    # solve K L1-regularized regression problem using LARs algorithm with cardinality constraint being d\n",
    "    n_sample, n_feature = X.shape\n",
    "    W = np.zeros((n_feature, n_clusters))\n",
    "    for i in range(n_clusters):\n",
    "        clf = linear_model.Lars(n_nonzero_coefs=n_selected_features)\n",
    "        clf.fit(X, Y[:, i])\n",
    "        W[:, i] = clf.coef_\n",
    "    return W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Feature Selection SPEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "\n",
    "class ScoreSelectorMixin:\n",
    "    \"\"\"\n",
    "    Mixin that adds getter for calculation of scores of features and checks that scores are calculated\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_scores(self):\n",
    "        \"\"\"\n",
    "        Get calculated scores for features\n",
    "        Returns\n",
    "        ------\n",
    "        scores: ndarray\n",
    "            Numpy Array with length equal to the initial number of features in dataset. Every element\n",
    "            of the array is the score of this feature\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _check_scores_set(self):\n",
    "        \"\"\"\n",
    "        Checks that scores for features are computed\n",
    "        Returns\n",
    "        -------\n",
    "        check: bool\n",
    "            Flag that indicates that scores are computed\n",
    "        \"\"\"\n",
    "\n",
    "        return self._get_scores() is not None\n",
    "\n",
    "\n",
    "class ThresholdSelectorMixin(ScoreSelectorMixin):\n",
    "    \"\"\"\n",
    "    Mixin that selects features according to some threshold. That means that\n",
    "    all features whose score is higher than threshold are selected\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_threshold(self):\n",
    "        \"\"\"\n",
    "        Get the value of threshold\n",
    "        Returns\n",
    "        -------\n",
    "        threshold: float\n",
    "            The value of threshold\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _get_support_mask(self):\n",
    "        if not self._check_scores_set():\n",
    "            raise NotFittedError('Feature Selector is not fitted')\n",
    "        return self._get_scores() > self._get_threshold()\n",
    "\n",
    "\n",
    "class KBestSelectorMixin(ScoreSelectorMixin):\n",
    "    \"\"\"\n",
    "    Mixin that selects K best features according to their scores\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_k(self):\n",
    "        \"\"\"\n",
    "        Get number of features to select\n",
    "        Returns\n",
    "        -------\n",
    "        k: int\n",
    "            Number of features to select\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _get_support_mask(self):\n",
    "        if not self._check_scores_set():\n",
    "            raise NotFittedError('Feature Selector is not fitted')\n",
    "        mask = np.zeros(self._get_scores().shape, dtype=bool)\n",
    "        mask[np.argsort(self._get_scores(), kind=\"mergesort\")[-self._get_k():]] = 1\n",
    "        return mask\n",
    "from abc import abstractmethod\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.feature_selection.base import SelectorMixin\n",
    "from sklearn.base import ClusterMixin\n",
    "\n",
    "\n",
    "class BaseFeatureSelector(BaseEstimator, SelectorMixin):\n",
    "    \"\"\"\n",
    "    Base class for all feature selection algorithms. It's SKLearn-compliant, so it can be used in pipelines.\n",
    "    Successors should override methods :meth:`fit`  and :meth:`_get_support_mask`.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, x, *rest):\n",
    "        \"\"\"\n",
    "        Fit selector to a dataset.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: ndarray\n",
    "            The input samples - array of shape [n_samples, n_features].\n",
    "        rest: list\n",
    "            List of miscellaneous arguments.\n",
    "        Returns\n",
    "        -------\n",
    "        selector: BaseFeatureSelector\n",
    "            Returns self to support chaining.\n",
    "        \"\"\"\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "class KBestFeatureSelector(KBestSelectorMixin, BaseFeatureSelector):\n",
    "    \"\"\"\n",
    "    Base class for algorithms that selects K best features according to features scores.\n",
    "    Successors should override method :meth:`_calc_scores` for computation of the score.\n",
    "    Parameters\n",
    "    ----------\n",
    "    k: int\n",
    "        Number of features to select\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.scores = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def _calc_scores(self, x):\n",
    "        \"\"\"\n",
    "        Calculate scores for features in dataset.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: ndarray\n",
    "            The input samples - array of shape [n_samples, n_features].\n",
    "        Returns\n",
    "        -------\n",
    "        scores: ndarray\n",
    "            Array of shape [n_features]. i-th element is a score of i-th feature.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fit(self, x, *rest):\n",
    "        self.scores = self._calc_scores(x)\n",
    "        return self\n",
    "\n",
    "    def _get_k(self):\n",
    "        return self.k\n",
    "\n",
    "    def _get_scores(self):\n",
    "        return self.scores\n",
    "\n",
    "\n",
    "class ClusteringFeatureSelector(KBestSelectorMixin, BaseFeatureSelector, ClusterMixin):\n",
    "    \"\"\"\n",
    "    Clusters samples and simultaneously finds relevant features. Allows to transform dataset\n",
    "    and select K best features according to features scores.\n",
    "    Parameters\n",
    "    ----------\n",
    "    k: int\n",
    "        Number of features to select\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.scores = None\n",
    "        self.labels_ = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def _calc_scores_and_labels(self, x):\n",
    "        \"\"\"\n",
    "        Calculate scores and labels for samples.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: ndarray\n",
    "            The input samples - array of shape [n_samples, n_features].\n",
    "        Returns\n",
    "        -------\n",
    "        scores_and_labels: tuple\n",
    "            Tuple where first element is scores of features and second element are cluster labels for samples\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fit(self, x, *rest):\n",
    "        scores, labels = self._calc_scores_and_labels(x)\n",
    "        self.scores = scores\n",
    "        self.labels_ = labels\n",
    "        return self\n",
    "\n",
    "    def _get_scores(self):\n",
    "        return self.scores\n",
    "\n",
    "    def _get_k(self):\n",
    "        return self.k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import rbf_kernel, cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "class SPEC(KBestFeatureSelector):\n",
    "    \"\"\"\n",
    "    SPEC-family of feature selection algorithms.\n",
    "\n",
    "    Every algorithm of this family creates graph representation of distribution of samples in a high-dimensional space.\n",
    "    Samples becomes vertices and RBF-distance between them becomes weight of the edge. Algorithms use Spectral Graph\n",
    "    Theory to calculate features scores.\n",
    "\n",
    "    Based on the article `\"Spectral feature selection for supervised and unsupervised learning.\" <http://www.public.asu.edu/~huanliu/papers/icml07.pdf>`_.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def _calc_spec_scores(self, degree, laplacian, normalised_features, normaliser):\n",
    "        \"\"\"\n",
    "        Calculate SPEC scores for the method\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        degree: ndarray\n",
    "            Degree matrix of the graph\n",
    "        laplacian: ndarray\n",
    "            Laplacian of the graph\n",
    "        normalised_features: ndarray\n",
    "            Feature vectors, normalised by normaliser\n",
    "        normaliser: ndarray\n",
    "            Vector computed by the degree matrix and used for normalisation of feature vectors and laplacian\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        scores: ndarray\n",
    "            Scores of features\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _calc_scores(self, x):\n",
    "        similarity = rbf_kernel(x)\n",
    "        adjacency = similarity\n",
    "        degree_vector = np.sum(adjacency, 1)\n",
    "        degree = np.diag(degree_vector)\n",
    "        laplacian = degree - adjacency\n",
    "        normaliser_vector = np.reciprocal(np.sqrt(degree_vector))\n",
    "        normaliser = np.diag(normaliser_vector)\n",
    "\n",
    "        normalised_laplacian = normaliser.dot(laplacian).dot(normaliser)\n",
    "\n",
    "        weighted_features = np.matmul(normaliser, x)\n",
    "\n",
    "        normalised_features = weighted_features / np.linalg.norm(weighted_features, axis=0)\n",
    "        return self._calc_spec_scores(degree, normalised_laplacian, normalised_features, normaliser)\n",
    "\n",
    "\n",
    "\n",
    "class NormalizedCut(SPEC):\n",
    "    \"\"\"\n",
    "    Feature selection algorithm that represents samples as vertices of graph.\n",
    "    Weights of edges of this graph are equal to RBF-distances between points.\n",
    "    Algorithm attempts to find minimal cut in this graph.\n",
    "\n",
    "    Algorithm selects k features which were `separated` by this cut as they are considered to be better for explanation\n",
    "    of the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    k: int\n",
    "        Number of features to select\n",
    "    \"\"\"\n",
    "\n",
    "    def _calc_spec_scores(self, degree, laplacian, normalised_features, normaliser):\n",
    "        all_to_all = normalised_features.transpose().dot(laplacian).dot(normalised_features)\n",
    "        return np.diag(all_to_all)\n",
    "\n",
    "\n",
    "\n",
    "class GenericSPEC(NormalizedCut):\n",
    "    \"\"\"\n",
    "    Feature selection algorithm that represents samples as vertices of graph.\n",
    "    Weights of edges of this graph are equal to RBF-distances between points.\n",
    "\n",
    "    Algorithm uses Spectral Graph Theory to find features with the best separability.\n",
    "\n",
    "    To do this, algorithm finds the trivial eugenvector of the Laplacian of the graph and uses it to\n",
    "    normalise scores computed using :class:`NormalizedCut`. Such normalisation helps to improve\n",
    "    accuracy of feature selection according to the article SPEC family is based on.\n",
    "\n",
    "    Algorithm selects top k features according to this scores\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    k: int\n",
    "        Number of features to select\n",
    "    \"\"\"\n",
    "\n",
    "    def _calc_spec_scores(self, degree, laplacian, normalised_features, normaliser):\n",
    "        normalised_cut = super()._calc_spec_scores(\n",
    "            degree, laplacian, normalised_features, normaliser\n",
    "        )\n",
    "        trivial_eugenvector = normaliser.dot(np.ones([normalised_features.shape[0], 1]))\n",
    "        norm = 1 - normalised_features.transpose().dot(trivial_eugenvector).squeeze().transpose()\n",
    "\n",
    "        return normalised_cut / norm\n",
    "\n",
    "\n",
    "\n",
    "class FixedSPEC(SPEC):\n",
    "    \"\"\"\n",
    "    Feature selection algorithm that represents samples as vertices of graph.\n",
    "    Weights of edges of this graph are equal to RBF-distances between points.\n",
    "\n",
    "    Algorithm uses Spectral Graph Theory to find features with the best separability in assumption that\n",
    "    points are separated to the predefined number of clusters\n",
    "\n",
    "    To do this, algorithm finds eigenvectors corresponding to K smallest eigenvalues of the Laplacian of the graph,\n",
    "    except the trivial one, and uses cosine distance between them and feature vectors to detect the most explaining\n",
    "    features. Algorithm selects k features using this score.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    k: int\n",
    "        Number of features to select\n",
    "    clusters: int\n",
    "        Expected number of clusters\n",
    "    \"\"\"\n",
    "    def __init__(self, k, clusters):\n",
    "        super().__init__(k)\n",
    "        self.clusters = clusters\n",
    "\n",
    "    def _calc_spec_scores(self, degree, laplacian, normalised_features, normaliser):\n",
    "        values, vectors = np.linalg.eigh(laplacian)\n",
    "\n",
    "        result = np.zeros([normalised_features.shape[1]])\n",
    "        for k in range(1, self.clusters):\n",
    "            eigenvalue = values[k]\n",
    "            eigenvector = vectors[:, k]\n",
    "            cosines = np.zeros_like(result)\n",
    "            for i in range(normalised_features.shape[1]):\n",
    "                feature = normalised_features[:, i]\n",
    "                similarity = cosine_similarity([eigenvector], [feature])[0]\n",
    "                cosines[i] = similarity * similarity\n",
    "            result += (2 - eigenvalue) * cosines\n",
    "        return -result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
